{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('py_ml': conda)"
  },
  "interpreter": {
   "hash": "bb9485a4479c7d40780f500c7638d1a5dd26719470aa2ae80757c09309d4a930"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Glimpsing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from scipy.stats import ks_2samp\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pylab import *\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "import dabl\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest, GradientBoostingClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, LeakyReLU, Embedding, SpatialDropout1D,\n",
    "                                     BatchNormalization, Dropout, Concatenate, Reshape)\n",
    "from tensorflow.keras import callbacks, backend\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e00535e301a4de284d0428f99f09d3f"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('precision', 5)\n",
    "\n",
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Time       V1       V2       V3       V4       V5       V6       V7       V8       V9      V10  \\\n",
       "0   0.0 -1.35981 -0.07278  2.53635  1.37816 -0.33832  0.46239  0.23960  0.09870  0.36379  0.09079   \n",
       "1   0.0  1.19186  0.26615  0.16648  0.44815  0.06002 -0.08236 -0.07880  0.08510 -0.25543 -0.16697   \n",
       "2   1.0 -1.35835 -1.34016  1.77321  0.37978 -0.50320  1.80050  0.79146  0.24768 -1.51465  0.20764   \n",
       "3   1.0 -0.96627 -0.18523  1.79299 -0.86329 -0.01031  1.24720  0.23761  0.37744 -1.38702 -0.05495   \n",
       "4   2.0 -1.15823  0.87774  1.54872  0.40303 -0.40719  0.09592  0.59294 -0.27053  0.81774  0.75307   \n",
       "\n",
       "       V11      V12      V13      V14      V15      V16      V17      V18      V19      V20  \\\n",
       "0 -0.55160 -0.61780 -0.99139 -0.31117  1.46818 -0.47040  0.20797  0.02579  0.40399  0.25141   \n",
       "1  1.61273  1.06524  0.48910 -0.14377  0.63556  0.46392 -0.11480 -0.18336 -0.14578 -0.06908   \n",
       "2  0.62450  0.06608  0.71729 -0.16595  2.34586 -2.89008  1.10997 -0.12136 -2.26186  0.52498   \n",
       "3 -0.22649  0.17823  0.50776 -0.28792 -0.63142 -1.05965 -0.68409  1.96578 -1.23262 -0.20804   \n",
       "4 -0.82284  0.53820  1.34585 -1.11967  0.17512 -0.45145 -0.23703 -0.03819  0.80349  0.40854   \n",
       "\n",
       "       V21      V22      V23      V24      V25      V26      V27      V28  Amount  Class  \n",
       "0 -0.01831  0.27784 -0.11047  0.06693  0.12854 -0.18911  0.13356 -0.02105  149.62      0  \n",
       "1 -0.22578 -0.63867  0.10129 -0.33985  0.16717  0.12589 -0.00898  0.01472    2.69      0  \n",
       "2  0.24800  0.77168  0.90941 -0.68928 -0.32764 -0.13910 -0.05535 -0.05975  378.66      0  \n",
       "3 -0.10830  0.00527 -0.19032 -1.17558  0.64738 -0.22193  0.06272  0.06146  123.50      0  \n",
       "4 -0.00943  0.79828 -0.13746  0.14127 -0.20601  0.50229  0.21942  0.21515   69.99      0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.35981</td>\n      <td>-0.07278</td>\n      <td>2.53635</td>\n      <td>1.37816</td>\n      <td>-0.33832</td>\n      <td>0.46239</td>\n      <td>0.23960</td>\n      <td>0.09870</td>\n      <td>0.36379</td>\n      <td>0.09079</td>\n      <td>-0.55160</td>\n      <td>-0.61780</td>\n      <td>-0.99139</td>\n      <td>-0.31117</td>\n      <td>1.46818</td>\n      <td>-0.47040</td>\n      <td>0.20797</td>\n      <td>0.02579</td>\n      <td>0.40399</td>\n      <td>0.25141</td>\n      <td>-0.01831</td>\n      <td>0.27784</td>\n      <td>-0.11047</td>\n      <td>0.06693</td>\n      <td>0.12854</td>\n      <td>-0.18911</td>\n      <td>0.13356</td>\n      <td>-0.02105</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.19186</td>\n      <td>0.26615</td>\n      <td>0.16648</td>\n      <td>0.44815</td>\n      <td>0.06002</td>\n      <td>-0.08236</td>\n      <td>-0.07880</td>\n      <td>0.08510</td>\n      <td>-0.25543</td>\n      <td>-0.16697</td>\n      <td>1.61273</td>\n      <td>1.06524</td>\n      <td>0.48910</td>\n      <td>-0.14377</td>\n      <td>0.63556</td>\n      <td>0.46392</td>\n      <td>-0.11480</td>\n      <td>-0.18336</td>\n      <td>-0.14578</td>\n      <td>-0.06908</td>\n      <td>-0.22578</td>\n      <td>-0.63867</td>\n      <td>0.10129</td>\n      <td>-0.33985</td>\n      <td>0.16717</td>\n      <td>0.12589</td>\n      <td>-0.00898</td>\n      <td>0.01472</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.35835</td>\n      <td>-1.34016</td>\n      <td>1.77321</td>\n      <td>0.37978</td>\n      <td>-0.50320</td>\n      <td>1.80050</td>\n      <td>0.79146</td>\n      <td>0.24768</td>\n      <td>-1.51465</td>\n      <td>0.20764</td>\n      <td>0.62450</td>\n      <td>0.06608</td>\n      <td>0.71729</td>\n      <td>-0.16595</td>\n      <td>2.34586</td>\n      <td>-2.89008</td>\n      <td>1.10997</td>\n      <td>-0.12136</td>\n      <td>-2.26186</td>\n      <td>0.52498</td>\n      <td>0.24800</td>\n      <td>0.77168</td>\n      <td>0.90941</td>\n      <td>-0.68928</td>\n      <td>-0.32764</td>\n      <td>-0.13910</td>\n      <td>-0.05535</td>\n      <td>-0.05975</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.96627</td>\n      <td>-0.18523</td>\n      <td>1.79299</td>\n      <td>-0.86329</td>\n      <td>-0.01031</td>\n      <td>1.24720</td>\n      <td>0.23761</td>\n      <td>0.37744</td>\n      <td>-1.38702</td>\n      <td>-0.05495</td>\n      <td>-0.22649</td>\n      <td>0.17823</td>\n      <td>0.50776</td>\n      <td>-0.28792</td>\n      <td>-0.63142</td>\n      <td>-1.05965</td>\n      <td>-0.68409</td>\n      <td>1.96578</td>\n      <td>-1.23262</td>\n      <td>-0.20804</td>\n      <td>-0.10830</td>\n      <td>0.00527</td>\n      <td>-0.19032</td>\n      <td>-1.17558</td>\n      <td>0.64738</td>\n      <td>-0.22193</td>\n      <td>0.06272</td>\n      <td>0.06146</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.15823</td>\n      <td>0.87774</td>\n      <td>1.54872</td>\n      <td>0.40303</td>\n      <td>-0.40719</td>\n      <td>0.09592</td>\n      <td>0.59294</td>\n      <td>-0.27053</td>\n      <td>0.81774</td>\n      <td>0.75307</td>\n      <td>-0.82284</td>\n      <td>0.53820</td>\n      <td>1.34585</td>\n      <td>-1.11967</td>\n      <td>0.17512</td>\n      <td>-0.45145</td>\n      <td>-0.23703</td>\n      <td>-0.03819</td>\n      <td>0.80349</td>\n      <td>0.40854</td>\n      <td>-0.00943</td>\n      <td>0.79828</td>\n      <td>-0.13746</td>\n      <td>0.14127</td>\n      <td>-0.20601</td>\n      <td>0.50229</td>\n      <td>0.21942</td>\n      <td>0.21515</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Loading Data and # Viewing Raw Data \n",
    "fraud_data = pd.read_csv('Datasets/creditcard.csv')\n",
    "fraud_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Dimension of data\n",
    "fraud_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Data type\n",
    "fraud_data.info()"
   ]
  },
  {
   "source": [
    "<font color='#00AA00'>\n",
    "Observation: <br />\n",
    "1. There are 29 input variables and 1 output variables (Class); <br />\n",
    "2. All the input variables is float type whereas the data type of out variable (Class) is int64; <br />\n",
    "3. No NaN variables or Null variables is found in the dataset as the non-null count of each variables match the total number of rows in the dataset.\n",
    "</font>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Time           V1           V2           V3           V4           V5           V6  \\\n",
       "count  284807.00000  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05   \n",
       "mean    94813.85958  3.91956e-15  5.68817e-16 -8.76907e-15  2.78231e-15 -1.55256e-15  2.01066e-15   \n",
       "std     47488.14595  1.95870e+00  1.65131e+00  1.51626e+00  1.41587e+00  1.38025e+00  1.33227e+00   \n",
       "min         0.00000 -5.64075e+01 -7.27157e+01 -4.83256e+01 -5.68317e+00 -1.13743e+02 -2.61605e+01   \n",
       "25%     54201.50000 -9.20373e-01 -5.98550e-01 -8.90365e-01 -8.48640e-01 -6.91597e-01 -7.68296e-01   \n",
       "50%     84692.00000  1.81088e-02  6.54856e-02  1.79846e-01 -1.98465e-02 -5.43358e-02 -2.74187e-01   \n",
       "75%    139320.50000  1.31564e+00  8.03724e-01  1.02720e+00  7.43341e-01  6.11926e-01  3.98565e-01   \n",
       "max    172792.00000  2.45493e+00  2.20577e+01  9.38256e+00  1.68753e+01  3.48017e+01  7.33016e+01   \n",
       "\n",
       "                V7           V8           V9          V10          V11          V12          V13  \\\n",
       "count  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05   \n",
       "mean  -1.69425e-15 -1.92703e-16 -3.13702e-15  1.76863e-15  9.17032e-16 -1.81066e-15  1.69344e-15   \n",
       "std    1.23709e+00  1.19435e+00  1.09863e+00  1.08885e+00  1.02071e+00  9.99201e-01  9.95274e-01   \n",
       "min   -4.35572e+01 -7.32167e+01 -1.34341e+01 -2.45883e+01 -4.79747e+00 -1.86837e+01 -5.79188e+00   \n",
       "25%   -5.54076e-01 -2.08630e-01 -6.43098e-01 -5.35426e-01 -7.62494e-01 -4.05571e-01 -6.48539e-01   \n",
       "50%    4.01031e-02  2.23580e-02 -5.14287e-02 -9.29174e-02 -3.27574e-02  1.40033e-01 -1.35681e-02   \n",
       "75%    5.70436e-01  3.27346e-01  5.97139e-01  4.53923e-01  7.39593e-01  6.18238e-01  6.62505e-01   \n",
       "max    1.20589e+02  2.00072e+01  1.55950e+01  2.37451e+01  1.20189e+01  7.84839e+00  7.12688e+00   \n",
       "\n",
       "               V14          V15          V16          V17          V18          V19          V20  \\\n",
       "count  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05   \n",
       "mean   1.47905e-15  3.48234e-15  1.39201e-15 -7.52849e-16  4.32877e-16  9.04973e-16  5.08550e-16   \n",
       "std    9.58596e-01  9.15316e-01  8.76253e-01  8.49337e-01  8.38176e-01  8.14041e-01  7.70925e-01   \n",
       "min   -1.92143e+01 -4.49894e+00 -1.41299e+01 -2.51628e+01 -9.49875e+00 -7.21353e+00 -5.44977e+01   \n",
       "25%   -4.25574e-01 -5.82884e-01 -4.68037e-01 -4.83748e-01 -4.98850e-01 -4.56299e-01 -2.11721e-01   \n",
       "50%    5.06013e-02  4.80715e-02  6.64133e-02 -6.56758e-02 -3.63631e-03  3.73482e-03 -6.24811e-02   \n",
       "75%    4.93150e-01  6.48821e-01  5.23296e-01  3.99675e-01  5.00807e-01  4.58949e-01  1.33041e-01   \n",
       "max    1.05268e+01  8.87774e+00  1.73151e+01  9.25353e+00  5.04107e+00  5.59197e+00  3.94209e+01   \n",
       "\n",
       "               V21          V22          V23          V24          V25          V26          V27  \\\n",
       "count  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05  2.84807e+05   \n",
       "mean   1.53729e-16  7.95991e-16  5.36759e-16  4.45811e-15  1.45300e-15  1.69910e-15 -3.66016e-16   \n",
       "std    7.34524e-01  7.25702e-01  6.24460e-01  6.05647e-01  5.21278e-01  4.82227e-01  4.03632e-01   \n",
       "min   -3.48304e+01 -1.09331e+01 -4.48077e+01 -2.83663e+00 -1.02954e+01 -2.60455e+00 -2.25657e+01   \n",
       "25%   -2.28395e-01 -5.42350e-01 -1.61846e-01 -3.54586e-01 -3.17145e-01 -3.26984e-01 -7.08395e-02   \n",
       "50%   -2.94502e-02  6.78194e-03 -1.11929e-02  4.09761e-02  1.65935e-02 -5.21391e-02  1.34215e-03   \n",
       "75%    1.86377e-01  5.28554e-01  1.47642e-01  4.39527e-01  3.50716e-01  2.40952e-01  9.10451e-02   \n",
       "max    2.72028e+01  1.05031e+01  2.25284e+01  4.58455e+00  7.51959e+00  3.51735e+00  3.16122e+01   \n",
       "\n",
       "               V28        Amount         Class  \n",
       "count  2.84807e+05  284807.00000  284807.00000  \n",
       "mean  -1.20605e-16      88.34962       0.00173  \n",
       "std    3.30083e-01     250.12011       0.04153  \n",
       "min   -1.54301e+01       0.00000       0.00000  \n",
       "25%   -5.29598e-02       5.60000       0.00000  \n",
       "50%    1.12438e-02      22.00000       0.00000  \n",
       "75%    7.82800e-02      77.16500       0.00000  \n",
       "max    3.38478e+01   25691.16000       1.00000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>284807.00000</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>2.84807e+05</td>\n      <td>284807.00000</td>\n      <td>284807.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>94813.85958</td>\n      <td>3.91956e-15</td>\n      <td>5.68817e-16</td>\n      <td>-8.76907e-15</td>\n      <td>2.78231e-15</td>\n      <td>-1.55256e-15</td>\n      <td>2.01066e-15</td>\n      <td>-1.69425e-15</td>\n      <td>-1.92703e-16</td>\n      <td>-3.13702e-15</td>\n      <td>1.76863e-15</td>\n      <td>9.17032e-16</td>\n      <td>-1.81066e-15</td>\n      <td>1.69344e-15</td>\n      <td>1.47905e-15</td>\n      <td>3.48234e-15</td>\n      <td>1.39201e-15</td>\n      <td>-7.52849e-16</td>\n      <td>4.32877e-16</td>\n      <td>9.04973e-16</td>\n      <td>5.08550e-16</td>\n      <td>1.53729e-16</td>\n      <td>7.95991e-16</td>\n      <td>5.36759e-16</td>\n      <td>4.45811e-15</td>\n      <td>1.45300e-15</td>\n      <td>1.69910e-15</td>\n      <td>-3.66016e-16</td>\n      <td>-1.20605e-16</td>\n      <td>88.34962</td>\n      <td>0.00173</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47488.14595</td>\n      <td>1.95870e+00</td>\n      <td>1.65131e+00</td>\n      <td>1.51626e+00</td>\n      <td>1.41587e+00</td>\n      <td>1.38025e+00</td>\n      <td>1.33227e+00</td>\n      <td>1.23709e+00</td>\n      <td>1.19435e+00</td>\n      <td>1.09863e+00</td>\n      <td>1.08885e+00</td>\n      <td>1.02071e+00</td>\n      <td>9.99201e-01</td>\n      <td>9.95274e-01</td>\n      <td>9.58596e-01</td>\n      <td>9.15316e-01</td>\n      <td>8.76253e-01</td>\n      <td>8.49337e-01</td>\n      <td>8.38176e-01</td>\n      <td>8.14041e-01</td>\n      <td>7.70925e-01</td>\n      <td>7.34524e-01</td>\n      <td>7.25702e-01</td>\n      <td>6.24460e-01</td>\n      <td>6.05647e-01</td>\n      <td>5.21278e-01</td>\n      <td>4.82227e-01</td>\n      <td>4.03632e-01</td>\n      <td>3.30083e-01</td>\n      <td>250.12011</td>\n      <td>0.04153</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.00000</td>\n      <td>-5.64075e+01</td>\n      <td>-7.27157e+01</td>\n      <td>-4.83256e+01</td>\n      <td>-5.68317e+00</td>\n      <td>-1.13743e+02</td>\n      <td>-2.61605e+01</td>\n      <td>-4.35572e+01</td>\n      <td>-7.32167e+01</td>\n      <td>-1.34341e+01</td>\n      <td>-2.45883e+01</td>\n      <td>-4.79747e+00</td>\n      <td>-1.86837e+01</td>\n      <td>-5.79188e+00</td>\n      <td>-1.92143e+01</td>\n      <td>-4.49894e+00</td>\n      <td>-1.41299e+01</td>\n      <td>-2.51628e+01</td>\n      <td>-9.49875e+00</td>\n      <td>-7.21353e+00</td>\n      <td>-5.44977e+01</td>\n      <td>-3.48304e+01</td>\n      <td>-1.09331e+01</td>\n      <td>-4.48077e+01</td>\n      <td>-2.83663e+00</td>\n      <td>-1.02954e+01</td>\n      <td>-2.60455e+00</td>\n      <td>-2.25657e+01</td>\n      <td>-1.54301e+01</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>54201.50000</td>\n      <td>-9.20373e-01</td>\n      <td>-5.98550e-01</td>\n      <td>-8.90365e-01</td>\n      <td>-8.48640e-01</td>\n      <td>-6.91597e-01</td>\n      <td>-7.68296e-01</td>\n      <td>-5.54076e-01</td>\n      <td>-2.08630e-01</td>\n      <td>-6.43098e-01</td>\n      <td>-5.35426e-01</td>\n      <td>-7.62494e-01</td>\n      <td>-4.05571e-01</td>\n      <td>-6.48539e-01</td>\n      <td>-4.25574e-01</td>\n      <td>-5.82884e-01</td>\n      <td>-4.68037e-01</td>\n      <td>-4.83748e-01</td>\n      <td>-4.98850e-01</td>\n      <td>-4.56299e-01</td>\n      <td>-2.11721e-01</td>\n      <td>-2.28395e-01</td>\n      <td>-5.42350e-01</td>\n      <td>-1.61846e-01</td>\n      <td>-3.54586e-01</td>\n      <td>-3.17145e-01</td>\n      <td>-3.26984e-01</td>\n      <td>-7.08395e-02</td>\n      <td>-5.29598e-02</td>\n      <td>5.60000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>84692.00000</td>\n      <td>1.81088e-02</td>\n      <td>6.54856e-02</td>\n      <td>1.79846e-01</td>\n      <td>-1.98465e-02</td>\n      <td>-5.43358e-02</td>\n      <td>-2.74187e-01</td>\n      <td>4.01031e-02</td>\n      <td>2.23580e-02</td>\n      <td>-5.14287e-02</td>\n      <td>-9.29174e-02</td>\n      <td>-3.27574e-02</td>\n      <td>1.40033e-01</td>\n      <td>-1.35681e-02</td>\n      <td>5.06013e-02</td>\n      <td>4.80715e-02</td>\n      <td>6.64133e-02</td>\n      <td>-6.56758e-02</td>\n      <td>-3.63631e-03</td>\n      <td>3.73482e-03</td>\n      <td>-6.24811e-02</td>\n      <td>-2.94502e-02</td>\n      <td>6.78194e-03</td>\n      <td>-1.11929e-02</td>\n      <td>4.09761e-02</td>\n      <td>1.65935e-02</td>\n      <td>-5.21391e-02</td>\n      <td>1.34215e-03</td>\n      <td>1.12438e-02</td>\n      <td>22.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>139320.50000</td>\n      <td>1.31564e+00</td>\n      <td>8.03724e-01</td>\n      <td>1.02720e+00</td>\n      <td>7.43341e-01</td>\n      <td>6.11926e-01</td>\n      <td>3.98565e-01</td>\n      <td>5.70436e-01</td>\n      <td>3.27346e-01</td>\n      <td>5.97139e-01</td>\n      <td>4.53923e-01</td>\n      <td>7.39593e-01</td>\n      <td>6.18238e-01</td>\n      <td>6.62505e-01</td>\n      <td>4.93150e-01</td>\n      <td>6.48821e-01</td>\n      <td>5.23296e-01</td>\n      <td>3.99675e-01</td>\n      <td>5.00807e-01</td>\n      <td>4.58949e-01</td>\n      <td>1.33041e-01</td>\n      <td>1.86377e-01</td>\n      <td>5.28554e-01</td>\n      <td>1.47642e-01</td>\n      <td>4.39527e-01</td>\n      <td>3.50716e-01</td>\n      <td>2.40952e-01</td>\n      <td>9.10451e-02</td>\n      <td>7.82800e-02</td>\n      <td>77.16500</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>172792.00000</td>\n      <td>2.45493e+00</td>\n      <td>2.20577e+01</td>\n      <td>9.38256e+00</td>\n      <td>1.68753e+01</td>\n      <td>3.48017e+01</td>\n      <td>7.33016e+01</td>\n      <td>1.20589e+02</td>\n      <td>2.00072e+01</td>\n      <td>1.55950e+01</td>\n      <td>2.37451e+01</td>\n      <td>1.20189e+01</td>\n      <td>7.84839e+00</td>\n      <td>7.12688e+00</td>\n      <td>1.05268e+01</td>\n      <td>8.87774e+00</td>\n      <td>1.73151e+01</td>\n      <td>9.25353e+00</td>\n      <td>5.04107e+00</td>\n      <td>5.59197e+00</td>\n      <td>3.94209e+01</td>\n      <td>2.72028e+01</td>\n      <td>1.05031e+01</td>\n      <td>2.25284e+01</td>\n      <td>4.58455e+00</td>\n      <td>7.51959e+00</td>\n      <td>3.51735e+00</td>\n      <td>3.16122e+01</td>\n      <td>3.38478e+01</td>\n      <td>25691.16000</td>\n      <td>1.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Summarizing data\n",
    "fraud_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Not Fraud    284315\n",
       "Fraud           492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Response Variable Analysis\n",
    "class_names = {0:'Not Fraud', 1:'Fraud'}\n",
    "fraud_data.Class.value_counts().rename(index = class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1017\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"set all random seed\"\"\"\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_feature_target(df: pd.DataFrame, target: str) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Split the dataset\n",
    "    :param df: raw dataset\n",
    "    :param target: name of target column\n",
    "    \"\"\"\n",
    "\n",
    "    x_ = df.drop(target, axis=1).values\n",
    "    y_ = df[target].values\n",
    "    y_ = y_.reshape(len(y_), 1)\n",
    "    return x_, y_\n",
    "\n",
    "def split_df(df: pd.DataFrame, random_seed: int) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split the dataset\n",
    "    :param df: raw dataset\n",
    "    :param random_seed: random seed\n",
    "    \"\"\"\n",
    "\n",
    "    # For Demo\n",
    "    train = df.sample(frac=0.8, random_state=random_seed)\n",
    "    dev_and_test = df.drop(train.index)\n",
    "    dev = dev_and_test.sample(frac=0.5, random_state=random_seed)\n",
    "    test = dev_and_test.drop(dev.index)\n",
    "\n",
    "    return train, dev, test\n",
    "\n",
    "def check_score(old_met, new_met):\n",
    "    print(f\"mean of top3 auc for old models: {round(old_met[:3]['auc'].mean(), 4)}\", \n",
    "          f\"new models: {round(new_met[:3]['auc'].mean(), 4)}\")\n",
    "    print(f\"mean of top3 rcal for old models: {round(old_met[:3]['recall'].mean(), 4)}\", \n",
    "          f\"new models: {round(new_met[:3]['recall'].mean(), 4)}\")\n",
    "\n",
    "def benchmark_models(train_x:np.array, dev_x:np.array, train_y:np.array, dev_y:np.array, \n",
    "                     models_dict: dict, random_s: int, suffix: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    fit models and return the model scores, without model tuning.\n",
    "\n",
    "    :param train_x, dev_x, train_y, dev_y: input data feature and target\n",
    "    :param models_dict: dictionary of target models\n",
    "    :param random_s: random seed\n",
    "    :param suffix: suffix for model name\n",
    "    :return: model score dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    #get score\n",
    "    cv_s = []\n",
    "    f1_ = []\n",
    "    auc_ = []\n",
    "    recall_ = []\n",
    "    acc_ = []\n",
    "    model_names = []\n",
    "    models = {}\n",
    "    # iterate train and fit model\n",
    "    for name, model in tqdm(models_dict.items()):\n",
    "        if suffix:\n",
    "            name = f\"{name}_{suffix}\"\n",
    "        else:\n",
    "            name = name\n",
    "        \n",
    "        # fit model\n",
    "        model.fit(train_x, train_y)\n",
    "        cv_score = cross_val_score(model, train_x, train_y, scoring='recall', \n",
    "                                   cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_s))\n",
    "        cv_s.append(cv_score.mean())\n",
    "\n",
    "        # dev validation\n",
    "        predict_y = model.predict(dev_x)\n",
    "        f1_.append(f1_score(dev_y, predict_y))\n",
    "        acc_.append(accuracy_score(dev_y, predict_y))\n",
    "        recall_.append(recall_score(dev_y, predict_y))\n",
    "\n",
    "        probability_y = model.predict_proba(dev_x)\n",
    "        auc_.append(roc_auc_score(dev_y, probability_y[:, 1]))\n",
    "\n",
    "        # save model\n",
    "        model_names.append(name)\n",
    "        models[name] = model\n",
    "    \n",
    "    # save val result\n",
    "    model_metrics = pd.DataFrame({\"model\": model_names, \"recall\": recall_, \"f1\": f1_, \"auc\": auc_,\n",
    "                                  \"accuracy\": acc_, \"train_cv\": cv_s})\n",
    "    model_metrics.sort_values([\"auc\",\"recall\"], inplace=True, ascending=False)\n",
    "\n",
    "    return model_metrics.set_index(\"model\"), models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 40%|████      | 4/10 [10:49<16:14, 162.44s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-23c63be4480a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mx_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_feature_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Class'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m raw_metrics, benchmark_model = benchmark_models(\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     models_dict={\"LReg\": LogisticRegression(),\n",
      "\u001b[1;32m<ipython-input-9-5d2739d9122f>\u001b[0m in \u001b[0;36mbenchmark_models\u001b[1;34m(train_x, dev_x, train_y, dev_y, models_dict, random_s, suffix)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         cv_score = cross_val_score(model, train_x, train_y, scoring='recall', \n\u001b[0m\u001b[0;32m     64\u001b[0m                                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_s))\n\u001b[0;32m     65\u001b[0m         \u001b[0mcv_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[0;32m    446\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    248\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[0;32m    249\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[1;32m--> 250\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    251\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    252\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m         \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[0;32m    685\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror_score\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scorers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_BaseScorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                 score = scorer._score(cached_call, estimator,\n\u001b[0m\u001b[0;32m     88\u001b[0m                                       *args, **kwargs)\n\u001b[0;32m     89\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \"\"\"\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod_caller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"predict\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;34m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffective_metric_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m             chunked_results = list(pairwise_distances_chunked(\n\u001b[0m\u001b[0;32m    706\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m                 \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1621\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1622\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1623\u001b[1;33m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0m\u001b[0;32m   1624\u001b[0m                                      n_jobs=n_jobs, **kwds)\n\u001b[0;32m   1625\u001b[0m         if ((X is Y or Y is None)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   1788\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1790\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py_ml\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# benchmark model with raw data\n",
    "\n",
    "# split the data\n",
    "train, dev, test = split_df(fraud_data, random_seed)\n",
    "\n",
    "x_train, y_train = split_feature_target(train, 'Class')\n",
    "x_dev, y_dev = split_feature_target(dev, 'Class')\n",
    "\n",
    "raw_metrics, benchmark_model = benchmark_models(\n",
    "    train_x=x_train, dev_x=x_dev, train_y=y_train, dev_y=y_dev,\n",
    "    models_dict={\"LReg\": LogisticRegression(),\n",
    "                 \"LR_balance\": LogisticRegression(class_weight=\"balanced\"),\n",
    "                 \"LR_balance_libl\": LogisticRegression(solver=\"liblinear\",\n",
    "                                                       class_weight=\"balanced\"),\n",
    "                 \"DecisionTree\": DecisionTreeClassifier(),\n",
    "                 \"KNN\": KNeighborsClassifier(),\n",
    "                 \"GaussianNB\": GaussianNB(),\n",
    "                 \"RandomForest\": RandomForestClassifier(n_estimators=50),\n",
    "                 \"XGB\": XGBClassifier(n_estimators=50, eval_metric=\"logloss\"),\n",
    "                 \"GBT\": GradientBoostingClassifier(n_estimators=50),\n",
    "                 \"LightGBM\": LGBMClassifier(n_estimators=50)},\n",
    "    random_s=random_seed)\n",
    "\n",
    "raw_metrics"
   ]
  },
  {
   "source": [
    "## EDA and Feature engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering by RandomForest\n",
    "feature_score = pd.DataFrame({'feature_score': benchmark_model[\"RandomForest\"].feature_importances_, \n",
    "                              'feature_name': train.columns[:-1]}).sort_values(\n",
    "                                  by='feature_score', ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x='feature_name', height='feature_score', data=feature_score)\n",
    "\n",
    "plt.title('feature_score')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature correlation\n",
    "def plot_corr_map(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plot the feature correlation\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(36,14))\n",
    "\n",
    "    # colleration matrix\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, annot=True)\n",
    "    plt.show()\n",
    "\n",
    "def get_large_correlation(df: pd.DataFrame, top_corr: int) -> None:\n",
    "    \"\"\"\n",
    "    Find top N correlations\n",
    "    :param df: input dataframe\n",
    "    :param top_corr: select top n correlation pairs\n",
    "    \"\"\"\n",
    "    # plot corr matrix\n",
    "    plot_corr_map(df)\n",
    "\n",
    "    print(\"Let's select some correlations automatically\")\n",
    "    # colleration pairs\n",
    "    all_corr = df.corr().abs().unstack()\n",
    "\n",
    "    # remove the diagonal/upper triangular pairs\n",
    "    drop_pairs = set()\n",
    "    corr_cols = df.columns\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(i+1):\n",
    "            drop_pairs.add((corr_cols[i], corr_cols[j]))\n",
    "    \n",
    "    # return large colleration pairs\n",
    "    all_corr.drop(labels=drop_pairs, inplace=True)\n",
    "    print(f\" Top {top_corr} Correlations \".center(60, '*'))\n",
    "    print(all_corr.sort_values(ascending=False)[:top_corr])\n",
    "    return None\n",
    "\n",
    "get_large_correlation(fraud_data, 20)"
   ]
  },
  {
   "source": [
    "<font color='#00AA00'>\n",
    "1. No variable pairs have strong correlation; <br />\n",
    "2. \"V2, Amount\", \"V5, Amount\", \"V7, Amount\" and \"V20 Amount\" have weak correlation between themselves; <br />\n",
    "3. \"V14, Class\" and \"V17, Class\" have weak correlation between themselves.\n",
    "</font>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numerical distribution\n",
    "def check_numerical_distribution(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"check the distribution and outlier from fraud and normal trx\"\"\"\n",
    "    quant1 = df[col].quantile(0.25)\n",
    "    quant3 = df[col].quantile(0.75)\n",
    "    std_r = (quant3 - quant1) * 1.5\n",
    "\n",
    "    indicate = df[col] < quant1 - std_r\n",
    "    indicate |= df[col] > quant3 + std_r\n",
    "\n",
    "    print(f\"for {col} distribution ratio between fraud and normal is \"\n",
    "          f\"{len(df.loc[(indicate) & (df['Class']==1)]) / len(df.loc[indicate])}\")\n",
    "\n",
    "for col in [\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V20\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"]:\n",
    "    check_numerical_distribution(fraud_data, col)"
   ]
  },
  {
   "source": [
    "<font color='#00AA00'>\n",
    "The distribution of the variables between fraud and normal trx is quite different.\n",
    "</font>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    @staticmethod\n",
    "    def remove_corr_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.drop(columns=[\"V2\", \"V3\", \"V5\", \"V7\", \"V20\"])\n",
    "\n",
    "class FeatureEngineering():\n",
    "    @staticmethod\n",
    "    def normalization(train_: pd.DataFrame, dev_: pd.DataFrame,\n",
    "                      test_: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\" normalze column values \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        cols_to_norm = [col for col in train_.columns if col not in [\"Class\"]]\n",
    "        train_[cols_to_norm] = scaler.fit_transform(train_[cols_to_norm])\n",
    "\n",
    "        dev_[cols_to_norm] = scaler.fit_transform(dev_[cols_to_norm])\n",
    "        test_[cols_to_norm] = scaler.fit_transform(test_[cols_to_norm])\n",
    "\n",
    "        return train_, dev_, test_\n",
    "        \n",
    "    @staticmethod\n",
    "    def new_feature(train_: pd.DataFrame, dev_: pd.DataFrame,\n",
    "                    test_: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        def _get_outlier_for_mean(train_, dev_, test_, col: str, new_col_name: str):\n",
    "            v_mean = train_[col].mean()\n",
    "            v_std = train_[col].std()\n",
    "            train_[new_col_name] = [0 if i < (v_mean-v_std) else 2 if i > (v_mean+v_std) else 1 for i in train_[col]]\n",
    "            dev_[new_col_name] = [0 if i < (v_mean-v_std) else 2 if i > (v_mean+v_std) else 1 for i in dev_[col]]\n",
    "            test_[new_col_name] = [0 if i < (v_mean-v_std) else 2 if i > (v_mean+v_std) else 1 for i in test_[col]]\n",
    "\n",
    "        def _get_outlier_2(train_, dev_, test_, col: str):\n",
    "            \"\"\"check the distribution and outlier from fraud and normal trx\"\"\"\n",
    "            quant1 = train_[col].quantile(0.25)\n",
    "            quant3 = train_[col].quantile(0.75)\n",
    "            std_r = (quant3 - quant1)*1.5\n",
    "\n",
    "            train_outlier = train_[col] < quant1 - std_r\n",
    "            train_outlier |= train_[col] > quant3 + std_r\n",
    "\n",
    "            quant1 = dev_[col].quantile(0.25)\n",
    "            quant3 = dev_[col].quantile(0.75)\n",
    "            std_r = (quant3 - quant1)*1.5\n",
    "            \n",
    "            dev_outlier = dev_[col] < quant1 - std_r\n",
    "            dev_outlier |= dev_[col] > quant3 + std_r\n",
    "\n",
    "            quant1 = test_[col].quantile(0.25)\n",
    "            quant3 = test_[col].quantile(0.75)\n",
    "            std_r = (quant3 - quant1)*1.5\n",
    "            \n",
    "            test_outlier = test_[col] < quant1 - std_r\n",
    "            test_outlier |= test_[col] > quant3 + std_r\n",
    "\n",
    "            new_col = col + \"_outlier\"\n",
    "            train_[new_col] = 0\n",
    "            dev_[new_col] = 0\n",
    "            test_[new_col] = 0\n",
    "\n",
    "            train_.loc[train_outlier.index, new_col] = 1\n",
    "            dev_.loc[dev_outlier.index, new_col] = 1\n",
    "            test_.loc[test_outlier.index, new_col] = 1\n",
    "\n",
    "        # =========================\n",
    "        # outlier for mean column\n",
    "\n",
    "        # outlier since these have different distribution between fraud and normal trx\n",
    "        for col in [\"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V20\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"]:\n",
    "            _get_outlier_2(train_, dev_, test_, col)\n",
    "        \n",
    "        return train_, dev_, test_\n",
    "        \n",
    "    @staticmethod\n",
    "    def resample_with_smote(df: pd.DataFrame, random_s: int) -> pd.DataFrame:\n",
    "        \"\"\"resample the train data\"\"\"\n",
    "        t_x = df.drop(columns=\"Class\")\n",
    "        t_y = df[[\"Class\"]]\n",
    "\n",
    "        t_x_new, t_y_new = SMOTE(random_state=random_s).fit_resample(t_x, t_y)\n",
    "        resample_df = t_x_new.join(t_y_new)\n",
    "        return resample_df\n",
    "        \n",
    "    @staticmethod\n",
    "    def down_sample(df: pd.DataFrame, random_s: int) -> pd.DataFrame:\n",
    "        \"\"\"randomly apply the undersampling\"\"\"\n",
    "        fraud_trx = df.loc[df[\"Class\"] == 1]\n",
    "        normal_trx = df.drop(fraud_trx.index)\n",
    "\n",
    "        normal_trx = resample(normal_trx, replace=False, n_samples=len(fraud_trx), random_state=random_s)\n",
    "\n",
    "        resample_df = pd.concat([fraud_trx, normal_trx])\n",
    "        return resample_df\n",
    "        \n",
    "    @staticmethod\n",
    "    def over_sample(df: pd.DataFrame, random_s: int) -> pd.DataFrame:\n",
    "        \"\"\"randomly apply the oversampling\"\"\"\n",
    "        fraud_trx = df.loc[df[\"Class\"] == 1]\n",
    "        normal_trx = df.drop(fraud_trx.index)\n",
    "\n",
    "        fraud_trx = resample(fraud_trx, replace=False, n_samples=len(normal_trx), random_state=random_s)\n",
    "\n",
    "        resample_df = pd.concat([fraud_trx, normal_trx])\n",
    "        return resample_df\n",
    "        \n",
    "    @staticmethod\n",
    "    def manual_feature(df: pd.DataFrame, random_s: int) -> pd.DataFrame:\n",
    "        \"\"\"creating some feature based on gut feeling\"\"\"\n",
    "\n",
    "        fraud_trx = df.loc[df[\"Class\"] == 1]\n",
    "        normal_trx = df.drop(fraud_trx.index)\n",
    "\n",
    "        normal_trx = resample(normal_trx, replace=False, n_samples=len(fraud_trx), random_state=random_s)\n",
    "\n",
    "        resample_df = pd.concat([fraud_trx, normal_trx])\n",
    "        return resample_df\n",
    "        \n",
    "    @staticmethod\n",
    "    def autoencoder_feature(x_tr: np.array, x_de: np.array, x_te: np.array, \n",
    "                            encode_size: int, random_s: int) -> Tuple[np.array, np.array, np.array]:\n",
    "        set_seed(random_s)\n",
    "        # define encoder\n",
    "        n_inputs = x_tr.shape[1]\n",
    "        print(f\"autoencoder n_inputs {n_inputs}, encode_size {encode_size}\")\n",
    "\n",
    "        visible = Input(shape=(n_inputs,))\n",
    "        # encoder level 1\n",
    "        e = Dense(n_inputs*2)(visible)\n",
    "        e = BatchNormalization()(e)\n",
    "        e = LeakyReLU()(e)\n",
    "\n",
    "        # bottleneck\n",
    "        n_bottleneck = encode_size\n",
    "        bottleneck = Dense(n_bottleneck)(e)\n",
    "\n",
    "        # decoder, level 1\n",
    "        d = Dense(n_inputs*2)(bottleneck)\n",
    "        d = BatchNormalization()(d)\n",
    "        d = LeakyReLU()(d)\n",
    "\n",
    "        # output layer\n",
    "        output = Dense(n_inputs, activation=\"linear\")(d)\n",
    "\n",
    "        # define autoencoder model\n",
    "        model = Model(inputs=visible, outputs=bottleneck)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        # fit the autoencoder model to reconstruct input\n",
    "        history = model.fit(x_tr, x_tr, epochs=80, batch_size=256, verbose=0, validation_data=(x_de, x_de))\n",
    "\n",
    "        # plot loss\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='val')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # define an encoder model (without the decoder)\n",
    "        encoder = Model(inputs=visible, outputs=bottleneck)\n",
    "\n",
    "        # encode the train data\n",
    "        x_train_encode = encoder.predict(x_tr)\n",
    "        x_train_encode = np.concatenate((x_tr, x_train_encode), axis=1)\n",
    "\n",
    "        # encode the dev data\n",
    "        x_dev_encode = encoder.predict(x_de)\n",
    "        x_dev_encode = np.concatenate((x_de, x_dev_encode), axis=1)\n",
    "\n",
    "        # encode the test data\n",
    "        x_test_encode = encoder.predict(x_te)\n",
    "        x_test_encode = np.concatenate((x_te, x_test_encode), axis=1)\n",
    "\n",
    "        return x_train_encode, x_dev_encode, x_test_encode\n",
    "\n",
    "def get_df(df: pd.DataFrame,\n",
    "           random_s: int,\n",
    "           drop_corr_col: bool=True,\n",
    "           #use_ohe_category: bool=True,\n",
    "           apply_normalization: bool=True,\n",
    "           #apply_log_trf: bool=False,\n",
    "           add_feature: bool=True,\n",
    "           resample: bool=True,\n",
    "           downsample: bool=False,\n",
    "           oversample: bool=False,\n",
    "           use_autoencoder: bool=False,\n",
    "           encode_size: int=8) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Data preprocessing\n",
    "    :param df: input df\n",
    "    :return: new df\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "\n",
    "    Prepo = Preprocessing()\n",
    "    FeatureE = FeatureEngineering()\n",
    "\n",
    "    # add new features (preprocess)\n",
    "    #if add_feature:\n",
    "    #    new_df = Prepo.add_feature(new_df)\n",
    "\n",
    "    # ohe_category variable\n",
    "    #if use_ohe_category:\n",
    "    #    new_df = Prepo.use_ohe_category(new_df)\n",
    "\n",
    "    # remove high correlation columns\n",
    "    if drop_corr_col:\n",
    "        new_df = Prepo.remove_corr_col(new_df)\n",
    "\n",
    "    #if apply_log_trf:\n",
    "    #    new_df = Prepo.apply_log(new_df)\n",
    "                 \n",
    "    # ==========================\n",
    "    # split data\n",
    "    train, dev, test = split_df(new_df, random_s)\n",
    "\n",
    "    # add new features \n",
    "    if add_feature:\n",
    "        train, dev, test = FeatureE.new_feature(train, dev, test)\n",
    "\n",
    "    # smote resample data\n",
    "    if resample:\n",
    "        downsample=False\n",
    "        train = FeatureE.resample_with_smote(train, random_s)\n",
    "\n",
    "    if downsample and resample == False:\n",
    "        oversample=False\n",
    "        train = FeatureE.down_sample(train, random_s)\n",
    "\n",
    "    if oversample:\n",
    "        train = FeatureE.over_sample(train, random_s)\n",
    "\n",
    "    # scaler\n",
    "    if apply_normalization:\n",
    "        train, dev, test = FeatureE.normalization(train, dev, test)\n",
    "\n",
    "    # df to array as input\n",
    "    x_train, y_train = split_feature_target(train, 'Class')\n",
    "    x_dev, y_dev = split_feature_target(dev, 'Class')\n",
    "    x_test, y_test = split_feature_target(test, 'Class')\n",
    "\n",
    "    # apply autoencoder\n",
    "    if use_autoencoder:\n",
    "        x_train, x_dev, x_test = FeatureE.autoencoder_feature(\n",
    "            x_train, x_dev, x_test, encode_size=encode_size, random_s=random_seed\n",
    "        )\n",
    "    \n",
    "    return x_train, y_train, x_dev, y_dev, x_test, y_test"
   ]
  },
  {
   "source": [
    "## Check model performance with/without engineering and resampling tech"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if remove variance / scale / without_resample\n",
    "x_train, y_train, x_dev, y_dev, x_test, y_test = get_df(fraud_data, random_s=random_seed, add_feature=False,\n",
    "                                                        resample=False, downsample=False)\n",
    "\n",
    "metrics_sc_rm_var_rm_col_ds, _ = benchmark_models(\n",
    "    train_x=x_train, dev_x=x_dev, train_y=y_train, dev_y=y_dev,\n",
    "    suffix=\"sc_rm_var_rm_col_ds\",\n",
    "    models_dict={\"LReg\": LogisticRegression(),\n",
    "                 \"LR_balance\": LogisticRegression(class_weight=\"balanced\"),\n",
    "                 \"LR_balance_libl\": LogisticRegression(solver=\"liblinear\",\n",
    "                                                       class_weight=\"balanced\"),\n",
    "                 \"DecisionTree\": DecisionTreeClassifier(criterion=\"entropy\", max_depth=5),\n",
    "                 \"KNN\": KNeighborsClassifier(),\n",
    "                 \"GaussianNB\": GaussianNB(),\n",
    "                 \"RandomForest\": RandomForestClassifier(n_estimators=50),\n",
    "                 \"XGB\": XGBClassifier(n_estimators=50, eval_metric=\"logloss\"),\n",
    "                 \"GBT\": GradientBoostingClassifier(n_estimators=50),\n",
    "                 \"LightGBM\": LGBMClassifier(n_estimators=50)},\n",
    "    random_s=random_seed)\n",
    "\n",
    "check_score(raw_metrics, metrics_sc_rm_var_rm_col_ds)\n",
    "metrics_sc_rm_var_rm_col_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if remove variance / scale / downsample\n",
    "x_train, y_train, x_dev, y_dev, x_test, y_test = get_df(fraud_data, random_s=random_seed, add_feature=False,\n",
    "                                                        resample=False, oversample=True)\n",
    "\n",
    "metrics_sc_rm_var_rm_col_ds, _ = benchmark_models(\n",
    "    train_x=x_train, dev_x=x_dev, train_y=y_train, dev_y=y_dev,\n",
    "    suffix=\"sc_rm_var_rm_col_ds\",\n",
    "    models_dict={\"LReg\": LogisticRegression(),\n",
    "                 \"LR_balance\": LogisticRegression(class_weight=\"balanced\"),\n",
    "                 \"LR_balance_libl\": LogisticRegression(solver=\"liblinear\",\n",
    "                                                       class_weight=\"balanced\"),\n",
    "                 \"DecisionTree\": DecisionTreeClassifier(criterion=\"entropy\", max_depth=5),\n",
    "                 \"KNN\": KNeighborsClassifier(),\n",
    "                 \"GaussianNB\": GaussianNB(),\n",
    "                 \"RandomForest\": RandomForestClassifier(n_estimators=50),\n",
    "                 \"XGB\": XGBClassifier(n_estimators=50, eval_metric=\"logloss\"),\n",
    "                 \"GBT\": GradientBoostingClassifier(n_estimators=50),\n",
    "                 \"LightGBM\": LGBMClassifier(n_estimators=50)},\n",
    "    random_s=random_seed)\n",
    "\n",
    "check_score(raw_metrics, metrics_sc_rm_var_rm_col_ds)\n",
    "metrics_sc_rm_var_rm_col_ds"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train autoencoder for classification\n",
    "x_train, y_train, x_dev, y_dev, x_test, y_test = get_df(fraud_data, random_s=random_seed, add_feature=False,\n",
    "                                                        resample=False, oversample=True, use_autoencoder=True,\n",
    "                                                        encode_size=16)\n",
    "\n",
    "metrics_sc_rm_var_rm_col_ds_autoe, _ = benchmark_models(\n",
    "    train_x=x_train, dev_x=x_dev, train_y=y_train, dev_y=y_dev,\n",
    "    suffix=\"sc_rm_var_rm_col_ds\",\n",
    "    models_dict={\"LReg\": LogisticRegression(),\n",
    "                 \"LR_balance\": LogisticRegression(class_weight=\"balanced\"),\n",
    "                 \"LR_balance_libl\": LogisticRegression(solver=\"liblinear\",\n",
    "                                                       class_weight=\"balanced\"),\n",
    "                 \"DecisionTree\": DecisionTreeClassifier(criterion=\"entropy\", max_depth=5),\n",
    "                 \"KNN\": KNeighborsClassifier(),\n",
    "                 \"GaussianNB\": GaussianNB(),\n",
    "                 \"RandomForest\": RandomForestClassifier(n_estimators=50),\n",
    "                 \"XGB\": XGBClassifier(n_estimators=50, eval_metric=\"logloss\"),\n",
    "                 \"GBT\": GradientBoostingClassifier(n_estimators=50),\n",
    "                 \"LightGBM\": LGBMClassifier(n_estimators=50)},\n",
    "    random_s=random_seed)\n",
    "\n",
    "check_score(metrics_sc_rm_var_rm_col_ds, metrics_sc_rm_var_rm_col_ds_autoe)\n",
    "metrics_sc_rm_var_rm_col_ds_autoe"
   ]
  },
  {
   "source": [
    "## Try Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_data():\n",
    "    cat_fraud_data = fraud_data.copy()\n",
    "\n",
    "    # remove some correlation features\n",
    "    # \"V2\", \"V3\", \"V5\", \"V7\", \"V20\"\n",
    "    cat_fraud_data = cat_fraud_data[[\"Time\", \"V1\", \"V3\", \"V5\", \"V6\", \"V8\", \"V9\", \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\", \"Class\"]]\n",
    "\n",
    "    train, dev, _ = split_df(cat_fraud_data, random_seed)\n",
    "\n",
    "    cols_to_norm = [x for x in cat_fraud_data.columns if x not in [\"Class\"]]\n",
    "    scaler = StandardScaler()\n",
    "    train[cols_to_norm] = scaler.fit_transform(train[cols_to_norm])\n",
    "    dev[cols_to_norm] = scaler.fit_transform(dev[cols_to_norm])\n",
    "\n",
    "    # iterate each column, get new feature.\n",
    "    for col in cols_to_norm:\n",
    "        new_col = col + \"_range\"\n",
    "        max_v = train[col].max()\n",
    "        min_v = train[col].min()\n",
    "        std_v = train[col].std()\n",
    "        step_v = max(int(max(max_v-min_v, 2)) / 10, 1)\n",
    "\n",
    "        # change numeric to category\n",
    "        train[new_col] = pd.cut(train[col], range(int(min_v+std_v), int(max_v-std_v), int(step_v)))\n",
    "        train[new_col] = train[new_col].cat.codes\n",
    "        train.loc[train[new_col] > int(max_v-std_v), new_col] = train[new_col].max()+1\n",
    "        train.drop(columns=col, inplace=True)\n",
    "\n",
    "        dev[new_col] = pd.cut(dev[col], range(int(min_v+std_v), int(max_v-std_v), int(step_v)))\n",
    "        dev[new_col] = dev[new_col].cat.codes\n",
    "        dev.loc[dev[new_col] > int(max_v-std_v), new_col] = dev[new_col].max()+1\n",
    "        for i in dev[new_col].unique():\n",
    "            if i not in train[new_col].unique():\n",
    "                new_i = i\n",
    "                while new_i not in train[new_col].unique():\n",
    "                    new_i+=1\n",
    "                    dev.loc[dev[new_col]==i, new_col] = new_i\n",
    "        dev.drop(columns=col, inplace=True)\n",
    "\n",
    "    # labelencoder\n",
    "    train[\"indi\"] = 1\n",
    "    dev[\"indi\"] = 0\n",
    "    data = pd.concat([train, dev])\n",
    "    for col in [x for x in data.columns if x not in [\"Class\"]]:\n",
    "        lbl = LabelEncoder()\n",
    "        data[col] = lbl.fit_transform(data[col].fillna(\"-1\").astype(str).values)\n",
    "    \n",
    "    train = data.loc[data[\"indi\"]==1].drop(columns=\"indi\")\n",
    "    dev = data.loc[data[\"indi\"]==1].drop(columns=\"indi\")\n",
    "\n",
    "    return train, dev\n",
    "\n",
    "# auc for tf\n",
    "def auc(y_t, y_pred):\n",
    "    def fallback_auc(y_t, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_t, y_pred)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return tf.py_function(fallback_auc, (y_t, y_pred), tf.double)\n",
    "\n",
    "\n",
    "def create_model(df, cols):\n",
    "    # create embedding model\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    # each feature get itself embedding\n",
    "    for c in cols:\n",
    "        num_unique_values = int(df[c].nunique())\n",
    "        embed_dim = int(min(np.ceil((num_unique_values)/2), 8))\n",
    "\n",
    "        inp = Input(shape=(1,))\n",
    "        out = Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = SpatialDropout1D(0.2)(out)\n",
    "        out = Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "\n",
    "    # all feature together become one layer\n",
    "    x = Concatenate()(outputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(num_unique_values*2, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(num_unique_values*2, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # output\n",
    "    y = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def embedding_model():\n",
    "    # build embedding model for classfication\n",
    "\n",
    "    # get train and test data\n",
    "    X_train, X_test = get_cat_data()\n",
    "    features = [x for x in X_train.columns if x not in [\"Class\"]]\n",
    "\n",
    "    # init model\n",
    "    model = create_model(pd.concat([X_train, X_test]), features)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n",
    "\n",
    "    oof_preds = np.zeros((len(X_train)))\n",
    "    test_preds = np.zeros((len(X_test)))\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = X_train.Class.values\n",
    "    X_train = [X_train.loc[:, features].values[:, k] for k in range(X_train.loc[:, features].values.shape[1])]\n",
    "\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_test = X_test.Class.values\n",
    "    X_test = [X_test.loc[:, features].values[:, k] for k in range(X_test.loc[:, features].values.shape[1])]\n",
    "\n",
    "    # callback for early stop and reduce learning rate\n",
    "    estop = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=3,\n",
    "                                    verbose=0, mode=\"max\", baseline=None, restore_best_weights=True)\n",
    "    rlearn = callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=3, \n",
    "                                    min_lr=1e-5, mode='max', verbose=0)\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X_train, to_categorical(y_train), \n",
    "              validation_data=(X_test, to_categorical(y_test)),\n",
    "              verbose=1, batch_size=512, callbacks=[estop, rlearn], epochs=30)\n",
    "\n",
    "    # check auc score\n",
    "    test_pred = model.predit(X_test)[:, 1]\n",
    "    print(f\"AUC {roc_auc_score(y_test, test_pred)}\")\n",
    "    K.clear_session()\n",
    "\n",
    "embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}